const http = require('http');
const fs = require('fs');

// Fun√ß√£o para fazer requisi√ß√µes HTTP
function makeRequest(options, data) {
  return new Promise((resolve, reject) => {
    const req = http.request(options, (res) => {
      let body = '';
      res.on('data', (chunk) => body += chunk);
      res.on('end', () => {
        try {
          const json = JSON.parse(body);
          resolve({ status: res.statusCode, data: json });
        } catch (e) {
          resolve({ status: res.statusCode, data: body });
        }
      });
    });
    
    req.on('error', reject);
    
    if (data) {
      req.write(JSON.stringify(data));
    }
    
    req.end();
  });
}

// Teste completo do fluxo do AI Wizard via API
async function testCompleteFlow() {
  console.log('üöÄ TESTE COMPLETO DO FLUXO AI WIZARD\n');
  
  const baseURL = 'http://localhost:8768';
  
  try {
    // ETAPA 1: Verificar sa√∫de do servidor
    console.log('üìç ETAPA 1: Verificando servidor...');
    const healthResponse = await makeRequest({
      hostname: 'localhost',
      port: 8768,
      path: '/health',
      method: 'GET'
    });
    
    if (healthResponse.status === 200) {
      console.log('‚úÖ Servidor saud√°vel');
    } else {
      console.log('‚ùå Servidor n√£o saud√°vel');
      return;
    }
    
    // ETAPA 2: Testar cen√°rios completos
    console.log('\nü§ñ ETAPA 2: Testando cen√°rios completos...');
    
    const scenarios = [
      {
        name: 'Fine-tuning LLaMA 7B',
        description: 'Quero fazer fine-tuning de LLaMA 7B com LoRA para deploy em produ√ß√£o',
        expectedKeywords: ['training', 'RTX_4090', 'RTX_3090', 'A6000', 'QLoRA'],
        expectedWorkload: 'training'
      },
      {
        name: 'API Stable Diffusion XL',
        description: 'API de Stable Diffusion XL para alta qualidade e m√∫ltiplos usu√°rios',
        expectedKeywords: ['inference', 'RTX_4070_Ti', 'RTX_4080', 'RTX_3090', '16GB'],
        expectedWorkload: 'inference'
      },
      {
        name: 'LLM 70B Produ√ß√£o',
        description: 'LLM 70B para produ√ß√£o com vLLM serving',
        expectedKeywords: ['inference', 'A100', 'H100', '80GB', 'multi-GPU'],
        expectedWorkload: 'inference'
      },
      {
        name: 'Treinamento YOLOv8',
        description: 'Treinamento de modelo YOLOv8 para detec√ß√£o de objetos',
        expectedKeywords: ['training', 'RTX_4090', 'A6000', 'RTX_4080', '16GB'],
        expectedWorkload: 'training'
      },
      {
        name: 'Infer√™ncia LLaMA 13B',
        description: 'Quero rodar LLaMA 13B para infer√™ncia em produ√ß√£o',
        expectedKeywords: ['inference', 'RTX_4090', 'A6000', 'RTX_3090', '24GB'],
        expectedWorkload: 'inference'
      }
    ];
    
    let totalTests = scenarios.length;
    let passedTests = 0;
    const results = [];
    
    for (let i = 0; i < scenarios.length; i++) {
      const scenario = scenarios[i];
      console.log(`\nüß™ Cen√°rio ${i + 1}/${totalTests}: ${scenario.name}`);
      console.log(`üìù Descri√ß√£o: ${scenario.description}`);
      
      const startTime = Date.now();
      
      // Fazer requisi√ß√£o para o AI Wizard
      const response = await makeRequest({
        hostname: 'localhost',
        port: 8768,
        path: '/api/v1/ai-wizard/analyze',
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        }
      }, {
        project_description: scenario.description,
        conversation_history: null
      });
      
      const responseTime = Date.now() - startTime;
      
      if (response.status === 200 && response.data.success) {
        console.log(`‚úÖ Status: ${response.status}`);
        console.log(`‚è±Ô∏è Tempo de resposta: ${responseTime}ms`);
        console.log(`ü§ñ Modelo: ${response.data.model_used}`);
        
        const recommendation = response.data.recommendation;
        
        // Verificar workload type
        const workloadCorrect = recommendation.workload_type === scenario.expectedWorkload;
        console.log(`üíº Workload: ${recommendation.workload_type} ${workloadCorrect ? '‚úÖ' : '‚ùå'}`);
        
        // Verificar palavras-chave
        let foundKeywords = 0;
        const recommendationString = JSON.stringify(recommendation).toLowerCase();
        
        for (const keyword of scenario.expectedKeywords) {
          if (recommendationString.includes(keyword.toLowerCase())) {
            foundKeywords++;
            console.log(`‚úÖ Encontrado: ${keyword}`);
          } else {
            console.log(`‚ùå N√£o encontrado: ${keyword}`);
          }
        }
        
        const score = (foundKeywords / scenario.expectedKeywords.length) * 100;
        console.log(`üìä Score: ${score.toFixed(1)}% (${foundKeywords}/${scenario.expectedKeywords.length})`);
        
        // Verificar se pede mais informa√ß√µes
        if (response.data.needs_more_info) {
          console.log('‚ùì Pediu mais informa√ß√µes');
        }
        
        // Salvar resultado
        const result = {
          scenario: scenario.name,
          description: scenario.description,
          status: response.status,
          responseTime,
          modelUsed: response.data.model_used,
          workloadType: recommendation.workload_type,
          score,
          keywordsFound: foundKeywords,
          totalKeywords: scenario.expectedKeywords.length,
          passed: score >= 70
        };
        
        results.push(result);
        
        if (score >= 70) {
          console.log('üéâ Cen√°rio aprovado!');
          passedTests++;
        } else {
          console.log('‚ö†Ô∏è Cen√°rio reprovado');
        }
        
        // Exibir detalhes das recomenda√ß√µes
        if (recommendation.min_vram_gb) {
          console.log(`üéÆ VRAM m√≠nima: ${recommendation.min_vram_gb}GB`);
        }
        
        if (recommendation.recommended_gpus && recommendation.recommended_gpus.length > 0) {
          console.log(`üéÆ GPUs: ${recommendation.recommended_gpus.join(', ')}`);
        }
        
        if (recommendation.explanation) {
          console.log(`üí° Explica√ß√£o: ${recommendation.explanation.substring(0, 100)}...`);
        }
        
      } else {
        console.log(`‚ùå Status: ${response.status}`);
        console.log(`‚ùå Erro: ${response.data.detail || 'Erro desconhecido'}`);
        
        results.push({
          scenario: scenario.name,
          description: scenario.description,
          status: response.status,
          passed: false,
          error: response.data.detail || 'Erro desconhecido'
        });
      }
    }
    
    // ETAPA 3: Testar casos limite
    console.log('\nüîç ETAPA 3: Testando casos limite...');
    
    const edgeCases = [
      { name: 'Mensagem vazia', description: '' },
      { name: 'Mensagem curta', description: 'oi' },
      { name: 'Mensagem amb√≠gua', description: 'quero uma gpu' },
      { name: 'Mensagem muito espec√≠fica', description: 'Quero fazer fine-tuning de LLaMA 7B com QLoRA, batch size 32, learning rate 1e-4, por 100 epochs' }
    ];
    
    for (const edgeCase of edgeCases) {
      console.log(`\nüß™ Testando: ${edgeCase.name}`);
      
      const response = await makeRequest({
        hostname: 'localhost',
        port: 8768,
        path: '/api/v1/ai-wizard/analyze',
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        }
      }, {
        project_description: edgeCase.description,
        conversation_history: null
      });
      
      if (response.status === 200) {
        console.log('‚úÖ Respondeu corretamente');
        if (response.data.needs_more_info) {
          console.log('üí¨ Pediu mais informa√ß√µes (comportamento esperado)');
        }
      } else {
        console.log(`‚ùå Erro: ${response.status}`);
      }
    }
    
    // ETAPA 4: Testar conversa√ß√£o com hist√≥rico
    console.log('\nüí¨ ETAPA 4: Testando conversa√ß√£o com hist√≥rico...');
    
    const conversationTest = await makeRequest({
      hostname: 'localhost',
      port: 8768,
      path: '/api/v1/ai-wizard/analyze',
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      }
    }, {
      project_description: 'Quero fazer fine-tuning de LLaMA 7B com LoRA',
      conversation_history: [
        { role: 'user', content: 'Estou procurando uma GPU para treinamento' },
        { role: 'assistant', content: 'Para treinamento, recomendo GPUs com mais VRAM' },
        { role: 'user', content: 'Qual seria a melhor op√ß√£o custo-benef√≠cio?' }
      ]
    });
    
    if (conversationTest.status === 200 && conversationTest.data.success) {
      console.log('‚úÖ Conversa√ß√£o com hist√≥rico funcionou');
      console.log(`ü§ñ Modelo: ${conversationTest.data.model_used}`);
      
      // Verificar se considera o hist√≥rico
      const recommendation = conversationTest.data.recommendation;
      if (recommendation.workload_type === 'training') {
        console.log('‚úÖ Considerou hist√≥rico (identificou treinamento)');
      }
    } else {
      console.log('‚ùå Conversa√ß√£o com hist√≥rico falhou');
    }
    
    // ETAPA 5: An√°lise de qualidade
    console.log('\nüìä ETAPA 5: An√°lise de qualidade das respostas...');
    
    // Verificar se est√° usando LLM real ou fallback
    const llmResponses = results.filter(r => r.modelUsed);
    const usingFallback = llmResponses.every(r => r.modelUsed === 'fallback');
    
    if (usingFallback) {
      console.log('‚ö†Ô∏è Sistema est√° usando fallback heur√≠stico');
      console.log('üí° Para usar LLM real, configure OPENROUTER_API_KEY v√°lida');
    } else {
      console.log('‚úÖ Sistema est√° usando LLM real');
    }
    
    // Verificar performance
    const avgResponseTime = results.reduce((sum, r) => sum + (r.responseTime || 0), 0) / results.length;
    console.log(`‚è±Ô∏è Tempo m√©dio de resposta: ${avgResponseTime.toFixed(1)}ms`);
    
    // Verificar consist√™ncia
    const workloads = results.map(r => r.workloadType).filter(Boolean);
    const uniqueWorkloads = [...new Set(workloads)];
    console.log(`üéØ Workloads detectados: ${uniqueWorkloads.join(', ')}`);
    
    // Relat√≥rio final
    console.log('\nüìã RELAT√ìRIO FINAL');
    console.log('='.repeat(60));
    console.log(`‚úÖ Testes executados: ${totalTests}`);
    console.log(`‚úÖ Testes aprovados: ${passedTests}`);
    console.log(`üìä Taxa de sucesso: ${((passedTests / totalTests) * 100).toFixed(1)}%`);
    console.log(`‚è±Ô∏è Tempo m√©dio resposta: ${avgResponseTime.toFixed(1)}ms`);
    console.log(`ü§ñ Modelo usado: ${usingFallback ? 'Fallback heur√≠stico' : 'LLM real'}`);
    
    if (passedTests === totalTests) {
      console.log('üéâ TODOS OS TESTES APROVADOS!');
      console.log('üí° AI Wizard est√° funcionando perfeitamente.');
    } else if (passedTests >= totalTests * 0.8) {
      console.log('‚úÖ AI Wizard est√° funcionando bem.');
    } else {
      console.log('‚ö†Ô∏è AI Wizard precisa de melhorias.');
    }
    
    // Salvar relat√≥rio detalhado
    const report = {
      timestamp: new Date().toISOString(),
      testType: 'Complete Flow API Test',
      summary: {
        totalTests,
        passedTests,
        successRate: (passedTests / totalTests) * 100,
        avgResponseTime,
        modelUsed: usingFallback ? 'fallback' : 'llm'
      },
      scenarios: results,
      edgeCases: edgeCases.length,
      conversationTest: conversationTest.status === 200,
      recommendations: [
        usingFallback ? 'Configure OPENROUTER_API_KEY para usar LLM real' : 'LLM real est√° funcionando',
        avgResponseTime < 100 ? 'Performance excelente' : 'Performance aceit√°vel',
        passedTests === totalTests ? 'Sistema pronto para produ√ß√£o' : 'Sistema precisa de ajustes'
      ]
    };
    
    fs.writeFileSync('/tmp/ai-wizard-complete-flow-report.json', JSON.stringify(report, null, 2));
    console.log('\nüìã Relat√≥rio detalhado: /tmp/ai-wizard-complete-flow-report.json');
    
    console.log('\nüéâ TESTE COMPLETO CONCLU√çDO!');
    
  } catch (error) {
    console.error('‚ùå Erro durante o teste:', error.message);
  }
}

// Verificar servidor
async function checkServer() {
  try {
    const response = await makeRequest({
      hostname: 'localhost',
      port: 8768,
      path: '/health',
      method: 'GET'
    });
    return response.status === 200;
  } catch (error) {
    return false;
  }
}

// Executar
async function main() {
  const serverRunning = await checkServer();
  if (!serverRunning) {
    console.log('‚ùå Servidor n√£o est√° rodando em http://localhost:8768');
    console.log('üí° Inicie o servidor: cd /home/ubuntu/dumont-cloud && python -m uvicorn src.main:app --host 0.0.0.0 --port 8768');
    process.exit(1);
  }
  
  await testCompleteFlow();
}

if (require.main === module) {
  main().catch(console.error);
}
